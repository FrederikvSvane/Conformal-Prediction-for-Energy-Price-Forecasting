{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3407a0a3",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783bff57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      4\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m../\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Go up to src/ directory\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/personal/CI_for_energy_price_forecasting/src/notebooks/../models.py:27\u001b[39m\n\u001b[32m     23\u001b[39m         output = \u001b[38;5;28mself\u001b[39m.linear(lstm_out[:, -\u001b[32m1\u001b[39m, :]) \u001b[38;5;66;03m# lstm_out has shape (batch_size, sequence_length, hidden_size), so doing [:, -1, :] just says return entire batch, the LAST time step and all hidden features\u001b[39;00m\n\u001b[32m     24\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstatespace\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msarimax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SARIMAX\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpmdarima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_arima\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dump, load\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Go up to src/ directory\n",
    "from models import LSTM_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb9179",
   "metadata": {},
   "source": [
    "# Setting up data and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from dataset import EnergyPriceDataset, load_and_preprocess_energy_data\n",
    "\n",
    "df = load_and_preprocess_energy_data('../../data/energy_data.csv')\n",
    "\n",
    "feature_cols = [\n",
    "    'Hour', 'day_nr', 'week_nr', 'year', 'month',\n",
    "    'day_of_year_sin', 'day_of_year_cos',\n",
    "    'wind_forecast_dah_mw', 'consumption_forecast_dah_mw',\n",
    "    'temp_forecast_dah_celcius', 'temp_norm_celcius',\n",
    "    'heating_demand_interaction', 'temp_deviation',\n",
    "    'spot_lag1'\n",
    "]\n",
    "target_col = 'spot'\n",
    "\n",
    "split_idx = int(len(df) * 0.8)\n",
    "train_df = df[:split_idx]\n",
    "test_df = df[split_idx:]\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "train_features = scaler_X.fit_transform(train_df[feature_cols])\n",
    "train_targets = scaler_y.fit_transform(train_df[[target_col]])\n",
    "\n",
    "test_features = scaler_X.transform(test_df[feature_cols])\n",
    "test_targets = scaler_y.transform(test_df[[target_col]])\n",
    "\n",
    "sequence_length = 48 # 24 = one day, 168 = one week\n",
    "train_dataset = EnergyPriceDataset(train_features, train_targets, sequence_length)\n",
    "test_dataset = EnergyPriceDataset(test_features, test_targets, sequence_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26f534",
   "metadata": {},
   "source": [
    "# LSTM Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25f6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 219/219 [00:02<00:00, 101.53it/s, loss=0.0946]\n",
      "Epoch 2/50: 100%|██████████| 219/219 [00:01<00:00, 120.44it/s, loss=0.074] \n",
      "Epoch 3/50: 100%|██████████| 219/219 [00:01<00:00, 115.99it/s, loss=0.0677]\n",
      "Epoch 4/50: 100%|██████████| 219/219 [00:01<00:00, 120.82it/s, loss=0.0619]\n",
      "Epoch 5/50: 100%|██████████| 219/219 [00:01<00:00, 120.34it/s, loss=0.0569]\n",
      "Epoch 6/50: 100%|██████████| 219/219 [00:01<00:00, 109.64it/s, loss=0.0513]\n",
      "Epoch 7/50: 100%|██████████| 219/219 [00:01<00:00, 110.55it/s, loss=0.0458]\n",
      "Epoch 8/50: 100%|██████████| 219/219 [00:01<00:00, 117.10it/s, loss=0.0417] \n",
      "Epoch 9/50: 100%|██████████| 219/219 [00:02<00:00, 99.85it/s, loss=0.0393] \n",
      "Epoch 10/50: 100%|██████████| 219/219 [00:02<00:00, 107.06it/s, loss=0.039]  \n",
      "Epoch 11/50: 100%|██████████| 219/219 [00:01<00:00, 117.20it/s, loss=0.0381] \n",
      "Epoch 12/50: 100%|██████████| 219/219 [00:01<00:00, 119.11it/s, loss=0.0383]\n",
      "Epoch 13/50: 100%|██████████| 219/219 [00:01<00:00, 118.36it/s, loss=0.0384] \n",
      "Epoch 14/50: 100%|██████████| 219/219 [00:01<00:00, 119.46it/s, loss=0.0377] \n",
      "Epoch 15/50: 100%|██████████| 219/219 [00:01<00:00, 116.78it/s, loss=0.0396] \n",
      "Epoch 16/50: 100%|██████████| 219/219 [00:01<00:00, 112.05it/s, loss=0.0367] \n",
      "Epoch 17/50: 100%|██████████| 219/219 [00:01<00:00, 116.29it/s, loss=0.0384]\n",
      "Epoch 18/50: 100%|██████████| 219/219 [00:01<00:00, 115.36it/s, loss=0.0332] \n",
      "Epoch 19/50: 100%|██████████| 219/219 [00:01<00:00, 118.17it/s, loss=0.0395] \n",
      "Epoch 20/50: 100%|██████████| 219/219 [00:01<00:00, 119.01it/s, loss=0.0323] \n",
      "Epoch 21/50: 100%|██████████| 219/219 [00:01<00:00, 117.97it/s, loss=0.0352] \n",
      "Epoch 22/50: 100%|██████████| 219/219 [00:01<00:00, 118.74it/s, loss=0.032]  \n",
      "Epoch 23/50: 100%|██████████| 219/219 [00:01<00:00, 116.12it/s, loss=0.0264] \n",
      "Epoch 24/50: 100%|██████████| 219/219 [00:01<00:00, 117.45it/s, loss=0.0265]\n",
      "Epoch 25/50: 100%|██████████| 219/219 [00:01<00:00, 119.17it/s, loss=0.0276] \n",
      "Epoch 26/50: 100%|██████████| 219/219 [00:01<00:00, 118.78it/s, loss=0.0241] \n",
      "Epoch 27/50: 100%|██████████| 219/219 [00:01<00:00, 114.54it/s, loss=0.0214] \n",
      "Epoch 28/50: 100%|██████████| 219/219 [00:02<00:00, 104.66it/s, loss=0.0203] \n",
      "Epoch 29/50: 100%|██████████| 219/219 [00:01<00:00, 109.93it/s, loss=0.0228] \n",
      "Epoch 30/50: 100%|██████████| 219/219 [00:01<00:00, 117.29it/s, loss=0.0206] \n",
      "Epoch 31/50: 100%|██████████| 219/219 [00:01<00:00, 118.89it/s, loss=0.0316] \n",
      "Epoch 32/50: 100%|██████████| 219/219 [00:01<00:00, 117.36it/s, loss=0.0257] \n",
      "Epoch 33/50: 100%|██████████| 219/219 [00:01<00:00, 115.48it/s, loss=0.0239] \n",
      "Epoch 34/50: 100%|██████████| 219/219 [00:01<00:00, 118.78it/s, loss=0.033]  \n",
      "Epoch 35/50: 100%|██████████| 219/219 [00:01<00:00, 117.79it/s, loss=0.0168] \n",
      "Epoch 36/50: 100%|██████████| 219/219 [00:01<00:00, 118.15it/s, loss=0.0267] \n",
      "Epoch 37/50: 100%|██████████| 219/219 [00:01<00:00, 115.96it/s, loss=0.0211] \n",
      "Epoch 38/50: 100%|██████████| 219/219 [00:01<00:00, 118.36it/s, loss=0.0166] \n",
      "Epoch 39/50: 100%|██████████| 219/219 [00:01<00:00, 116.97it/s, loss=0.0193] \n",
      "Epoch 40/50: 100%|██████████| 219/219 [00:01<00:00, 112.45it/s, loss=0.0185] \n",
      "Epoch 41/50: 100%|██████████| 219/219 [00:01<00:00, 117.84it/s, loss=0.0158] \n",
      "Epoch 42/50: 100%|██████████| 219/219 [00:01<00:00, 117.81it/s, loss=0.0157] \n",
      "Epoch 43/50: 100%|██████████| 219/219 [00:01<00:00, 116.12it/s, loss=0.0114] \n",
      "Epoch 44/50: 100%|██████████| 219/219 [00:01<00:00, 114.67it/s, loss=0.0147] \n",
      "Epoch 45/50: 100%|██████████| 219/219 [00:01<00:00, 116.21it/s, loss=0.0139] \n",
      "Epoch 46/50: 100%|██████████| 219/219 [00:01<00:00, 116.00it/s, loss=0.0113] \n",
      "Epoch 47/50: 100%|██████████| 219/219 [00:01<00:00, 117.69it/s, loss=0.0152] \n",
      "Epoch 48/50: 100%|██████████| 219/219 [00:01<00:00, 116.00it/s, loss=0.0154] \n",
      "Epoch 49/50: 100%|██████████| 219/219 [00:01<00:00, 114.16it/s, loss=0.0143] \n",
      "Epoch 50/50: 100%|██████████| 219/219 [00:01<00:00, 114.55it/s, loss=0.0153] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = LSTM_model(input_size=14)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 31\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for X_batch, y_batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model.forward(X_batch)\n",
    "        loss = loss_func(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "# NOTE: this is retarded and approximate. \n",
    "# We find a proper epoch range by running expanding window cross validation \n",
    "# (see other script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45653bbf",
   "metadata": {},
   "source": [
    "# Save for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "torch.save(model.state_dict(), 'lstm_model.pth')\n",
    "pickle.dump((scaler_X, scaler_y), open('scalers.pkl', 'wb')) # when we want to undo the scaling of the data, we need to know the used scaling parameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
